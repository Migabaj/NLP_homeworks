{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Doc\n",
    "from natasha import NewsEmbedding\n",
    "from natasha import NewsMorphTagger\n",
    "from natasha import Segmenter\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "from string import punctuation\n",
    "punctuation += '” '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Я умел и я любим всеми, как мы любим. Раньше он умел печь. Кладу на печь. Тот сундук наносит вред тому кладу. Бог Тот не рад первому тому. Не надо никаких Верховных рад. Надо мною летает стих. Ты почему так резко стих? Где Катя? Катя машину до заправки, я пару раз упал. Машину слезу никто не видел? Сейчас слезу, блин! Довольно вкусный блин. Его эго довольно. Ной не ныл, а ты ной. О, понял! Песня о любви. Но я жал на кнопку. Много пчелиных жал. Жаль, пчела, чтобы было очень жаль! Желаю им совет да любовь! Да! Уже все, не поговорим об уже?*\n",
    "\n",
    "\n",
    "Почему этот текст окажется сложным для тэггера -- понятно: как видите, тут много омонимов, которые повторяются, но принадлежат к разным лексемам и разным частям речи. \"печь\", \"жаль\", \"машину\", \"надо\" и т.д. и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"russian_golden.txt\", encoding=\"utf-8\") as f:\n",
    "    russian_golden = f.read()\n",
    "with open(\"russian_golden_notes.txt\", encoding=\"utf-8\") as f:\n",
    "    russian_golden_grammemes = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "russian_golden_no_punct = russian_golden.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mystem_ana = m.analyze(russian_golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# граммемы майстема\n",
    "mystem_grammemes = []\n",
    "for token in mystem_ana:\n",
    "    if \"analysis\" in token:\n",
    "        gr = token['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        mystem_grammemes.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# граммемы пайморфи\n",
    "pymorphy_grammemes = []\n",
    "for token in russian_golden_no_punct.split():\n",
    "    pos = morph.parse(token)[0].tag.POS\n",
    "    pymorphy_grammemes.append(str(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(russian_golden)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# граммемы наташи\n",
    "natasha_grammemes = []\n",
    "for token in doc.tokens:\n",
    "    if token.pos != \"PUNCT\":\n",
    "        natasha_grammemes.append(token.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы можно было приравнять разные грамеммы, я создал словарь русских граммем. Ключи -- то, как я обозначил части речи у себя, значения -- то, как их по-разному могут обозначать парсеры (поскольку нам важна только часть речи, мы пренебрегаем тем, что в списке могут быть две грамеммы с разным смыслом, но обозначающие одну часть речи)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_golden_pos_dict = {\n",
    "    \"A\": [\"A\", \"ADJ\", \"ADJF\"],\n",
    "    \"ADV\": [\"ADV\", \"ADVB\"],\n",
    "    \"ANUM\": [\"ANUM\", \"A\", \"ADJ\"],\n",
    "    \"ADVPRO\": [\"ADVPRO\", \"ADV\", \"ADVB\", \"PRON\",],\n",
    "    \"APRO\": [\"APRO\", \"A\", \"PRON\", \"ADJ\", \"ADJF\"],\n",
    "    \"COMP\": [\"COMP\", \"ADV\"],\n",
    "    \"CONJ\": [\"CONJ\", \"CCONJ\",],\n",
    "    \"INTJ\": [\"INTJ\",],\n",
    "    \"PART\": [\"PART\", \"PRCL\"],\n",
    "    \"PR\": [\"PR\", \"ADP\", \"PREP\"],\n",
    "    \"S\": [\"S\", \"NOUN\",],\n",
    "    \"SPRO\": [\"SPRO\", \"S\", \"NOUN\", \"PRON\", \"NPRO\"],\n",
    "    \"V\": [\"V\", \"VERB\",],\n",
    "    \"DET\": [\"DET\", \"APRO\", \"A\", \"ADJ\", \"ADJF\"],\n",
    "    \"INFN\": [\"V\", \"VERB\", \"INFN\"],\n",
    "    \"ADJS\": [\"A\", \"ADJ\", \"ADJS\"],\n",
    "    \"PRED\": [\"PRED\", \"ADV\", \"ADVB\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшая функция для accuracy. на вход: список граммем, словарь, список \"золотых\" или правильных граммем (написанных вручную).\n",
    "На выходе -- доля правильных граммем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ana, dic, golden):\n",
    "    correct = 0\n",
    "    for pos, pos_golden in zip(ana, golden):\n",
    "        if pos in dic[pos_golden]:\n",
    "            correct += 1\n",
    "    return correct/len(golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7307692307692307"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(pymorphy_grammemes, rus_golden_pos_dict, russian_golden_grammemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7596153846153846"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(natasha_grammemes, rus_golden_pos_dict, russian_golden_grammemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8076923076923077"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(mystem_grammemes, rus_golden_pos_dict, russian_golden_grammemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, майстем значительно лучше остальных.\n",
    "\n",
    "Теперь посмотрим на английские парсеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 22:07:32,581 loading file /Users/fixed/.flair/models/en-pos-ontonotes-v0.5.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "\n",
    "tagger = SequenceTagger.load('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The complex houses married and single soldiers and their families. “I see,” said the blind man as he picked up the hammer and saw. The old man the boats. Will Will Smith smith? Do you mind if I change my mind? If you like it, give it a like! Leaves leave every fall. I’m feeling weird about having a feeling. I’m well, but I fell inside a well. I was the first one to coin the term coin. The bully wants to bully me. Love you, my love. I hate my fucking life. Are those guys fucking? I think that they are.*\n",
    "\n",
    "Похожая ситуация, что и в русском тексте: много омонимов, которые принадлежат разным частям речи: *love*, *feeling*, *leave*, *saw*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"english_golden.txt\", encoding=\"utf-8\") as f:\n",
    "    english_golden = f.read().strip('\\n')\n",
    "with open(\"english_golden_notes.txt\", encoding=\"utf-8\") as f:\n",
    "    english_golden_grammemes = f.read().strip('\\n').split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Sentence(english_golden)\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# граммемы flair\n",
    "flair_grammemes = []\n",
    "for entity in sentence.to_dict(tag_type=\"pos\")['entities']:\n",
    "    if entity['text'] not in punctuation:\n",
    "        flair_grammemes.append(str(entity['labels'][0]).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(english_golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# граммемы spacy\n",
    "spacy_grammemes = []\n",
    "for token in doc:\n",
    "    if token.text not in punctuation:\n",
    "        spacy_grammemes.append(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_nltk = word_tokenize(english_golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# граммемы nltk\n",
    "nltk_grammemes = []\n",
    "for token in nltk.pos_tag(tokens_nltk):\n",
    "    if token[0] not in punctuation:\n",
    "        nltk_grammemes.append(token[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем аналогичный словарь для английских граммем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_golden_pos_dict = {\n",
    "    \"DET\": [\"DT\", \"DET\"],\n",
    "    \"NOUN\": [\"NOUN\", \"NN\", \"NNP\", \"NNS\"],\n",
    "    \"ADJ\": [\"ADJ\", \"JJ\"],\n",
    "    \"VERB\": [\"VBP\", \"VBD\", \"VBG\", \"VBZ\", \"VB\", \"VERB\"],\n",
    "    \"CONJ\": [\"CC\", \"CCONJ\", \"SCONJ\", \"IN\"],\n",
    "    \"PRON\": [\"PRON\", \"PRP\"],\n",
    "    \"AUX\": [\"AUX\", \"VBP\", \"VBD\", \"VERB\"],\n",
    "    \"NUM\": [\"CD\", \"NUM\"],\n",
    "    \"PART\": [\"TO\", \"PART\"],\n",
    "    \"PRP$\": [\"PRP$\", \"DET\", \"DT\"],\n",
    "    \"ADP\": [\"ADP\", \"RP\", \"RB\", \"IN\"],\n",
    "    \"ADV\": [\"ADV\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция у нас уже есть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8653846153846154"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(spacy_grammemes, eng_golden_pos_dict, english_golden_grammemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(nltk_grammemes, eng_golden_pos_dict, english_golden_grammemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8846153846153846"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(flair_grammemes, eng_golden_pos_dict, english_golden_grammemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что в целом accuracy гораздо выше русской: самый низкий английский результат примерно на 6 процентов выше самого высокого русского.\n",
    "\n",
    "Для последнего пункта я выбрал три конструкции: \"совсем не A\", \"A, хотя A\" и \"ADV A\". Я предполагаю, что вторая и третья чаще встречаются в положительных отзывах (\"интересный, хотя длинноватый\" -- в положительных отзывах чаще можно встретить, как человек прощает недостаток, интуитивно кажется, что это вероятнее, чем если бы кто-то ругал фильм, но не забыл упомянуть плюс; \"удивительно легкий\", \"очень хороший\" кажутся более, а первая -- наоборот. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myst_pos(token):\n",
    "    if 'analysis' in token:\n",
    "        return token['analysis'][0]['gr'].split('=')[0].split(',')[0]\n",
    "\n",
    "def phrase_getter(text):\n",
    "    ana = m.analyze(text)\n",
    "    newtext = []\n",
    "    j = 0\n",
    "    \n",
    "    while ana:\n",
    "        \n",
    "        if myst_pos(ana[0]) == \"ADV\" and myst_pos(ana[2]) == \"A\":\n",
    "            newtext.append([tok['text'] for tok in ana[:3]])\n",
    "            ana = ana[3:]\n",
    "        elif ana[0]['text'] == \"совсем\" and ana[2][\"text\"] == \"не\" and myst_pos(ana[4]) == \"A\":\n",
    "            newtext.append([tok['text'] for tok in ana[:5]])\n",
    "            ana = ana[5:]\n",
    "        elif myst_pos(ana[0]) == \"A\" and ana[2]['text'] == 'хотя' and myst_pos(ana[4]) == \"A\":\n",
    "            newtext.append([tok['text'] for tok in ana[:5]])\n",
    "            ana = ana[5:]\n",
    "        else:\n",
    "            newtext.append(ana[0]['text'])\n",
    "            ana = ana[1:]\n",
    "        \n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['красивый', ', ', 'хотя', ' ', 'длинноватый'], ' ', 'фильм', '\\n']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_getter(\"красивый, хотя длинноватый фильм\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['очень', ' ', 'хорошие'], ' ', 'актеры', '\\n']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_getter(\"очень хорошие актеры\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', ' ', ['совсем', ' ', 'не', ' ', 'релеватные'], ' ', 'темы', '\\n']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_getter(\"на совсем не релеватные темы\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть, насколько хорошо это добавление влияет на первую домашку, я не успел :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Миша Сонькин*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

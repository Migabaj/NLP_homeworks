{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа по NLP №1\n",
    "\n",
    "Для начала загрузим все нужные модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "m = Mystem()\n",
    "punctuation += \"\\xa0—\\xa0\\n«»\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*tqdm* я загрузил для себя, он необязателен для выполнения задания.\n",
    "\n",
    "Закачиваем данные — последние отзывы из Кинопоиска на различные фильмы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799c1057f523404ca50587d91bb03456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# у нас будут четыре списка:\n",
    "# два для обработки и два для\n",
    "# тестирования.\n",
    "good_reviews = []\n",
    "bad_reviews = []\n",
    "good_reviews_test = []\n",
    "bad_reviews_test = []\n",
    "\n",
    "# цикл, создающий эти четыре списка\n",
    "for i in tqdm(range(25)):\n",
    "    \n",
    "    # при помощи requests закачиваем по странице.\n",
    "    # у кинопоиска есть разделы \"плохие отзывы\" и\n",
    "    # \"хорошие отзывы\", что нам сильно помогает при парсинге.\n",
    "    good_reviews_page = requests.get(f\"https://www.kinopoisk.ru/reviews/status/good/page/{str(i+1)}/#list\")\n",
    "    bad_reviews_page = requests.get(f\"https://www.kinopoisk.ru/reviews/status/bad/page/{str(i+1)}/#list\")\n",
    "    # варим суп из полученного html-а.\n",
    "    good_soup = BeautifulSoup(good_reviews_page.text, \"html.parser\")\n",
    "    bad_soup = BeautifulSoup(bad_reviews_page.text, \"html.parser\")\n",
    "    \n",
    "    # тексты отзыва лежат внутри тэга div\n",
    "    # класса brand_words.\n",
    "    good_reviews_html = good_soup.find_all(\"div\", {\"class\": \"brand_words\"})\n",
    "    bad_reviews_html = bad_soup.find_all(\"div\", {\"class\": \"brand_words\"})\n",
    "    \n",
    "    # первые 20 страниц отзывов возьмем для\n",
    "    # создания нашей модели, оставшиеся 5 —\n",
    "    # для тестирования\n",
    "    if i < 20:\n",
    "        # при помощи MyStem лемматизируем\n",
    "        # текст и выбрасываем все токены-знаки\n",
    "        # препинания. на выходе получаем список\n",
    "        # всех лемм.\n",
    "        for review in good_reviews_html:\n",
    "            lemmas = m.lemmatize(review.get_text())\n",
    "            good_reviews.extend([lemma for lemma in lemmas if not lemma.strip() in punctuation])\n",
    "        for review in bad_reviews_html:\n",
    "            lemmas = m.lemmatize(review.get_text())\n",
    "            bad_reviews.extend([lemma for lemma in lemmas if not lemma.strip() in punctuation])\n",
    "    else:\n",
    "        # здесь делаем то же самое, но нам\n",
    "        # вместо extend нужен append, т.к.\n",
    "        # мы рассматриваем отзывы\n",
    "        # по отдельности.\n",
    "        for review in good_reviews_html:\n",
    "            lemmas = m.lemmatize(review.get_text())\n",
    "            good_reviews_test.append([lemma for lemma in lemmas if not lemma.strip() in punctuation])\n",
    "        for review in bad_reviews_html:\n",
    "            lemmas = m.lemmatize(review.get_text())\n",
    "            bad_reviews_test.append([lemma for lemma in lemmas if not lemma.strip() in punctuation])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем четыре списка, содержимое которых выглядит примерно вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['история',\n",
       " 'ничто',\n",
       " 'не',\n",
       " 'учить',\n",
       " 'даже',\n",
       " 'хороший',\n",
       " 'мир',\n",
       " 'сей',\n",
       " 'кастинг',\n",
       " 'это']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_reviews[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['после',\n",
       " 'просмотр',\n",
       " 'данный',\n",
       " 'фильм',\n",
       " 'я',\n",
       " 'становиться',\n",
       " 'понятно',\n",
       " 'что',\n",
       " 'ничто',\n",
       " 'не']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_reviews_test[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим функцию, которая создаст для нас два нужных нам множества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# на входе: два \"корпуса\" слов.\n",
    "def unique_words(corpus1, corpus2):\n",
    "    # множества слов, уникальных для\n",
    "    # положительных/отрицательных отзывов.\n",
    "    corpus1_unique = set()\n",
    "    corpus2_unique = set()\n",
    "    \n",
    "    # счетчики, нужные для того, чтобы\n",
    "    # отбросить слова с низкой частотностью.\n",
    "    corpus1_cnt = Counter(corpus1)\n",
    "    corpus2_cnt = Counter(corpus2)\n",
    "    \n",
    "    # из счетчиков создаем сеты, в которых\n",
    "    # оставляем только те слова, которые\n",
    "    # встречаются больше пяти раз.\n",
    "    corpus1_tokens = set([k for k,v in corpus1_cnt.items() if v > 5])\n",
    "    corpus2_tokens = set([k for k,v in corpus2_cnt.items() if v > 5])\n",
    "    \n",
    "    # проходим по двум сетам, чтобы\n",
    "    # пополнить сеты уникальных слов.\n",
    "    for token in corpus1_tokens:\n",
    "        if token not in corpus2_tokens:\n",
    "            corpus1_unique.add(token)\n",
    "    for token in corpus2_tokens:\n",
    "        if token not in corpus1_tokens:\n",
    "            corpus2_unique.add(token)\n",
    "    \n",
    "    # на выходе: кортеж из двух\n",
    "    # множеств уникальных слов.\n",
    "    return (corpus1_unique, corpus2_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем функцию на двух \"словарях\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique = unique_words(good_reviews, bad_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая посчитает в тексте (списке лемм) кол-во \"положительных\" слов и предположит его тональность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# на входе: отзыв-набор лемм.\n",
    "def review_tone(review):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    # для каждой леммы смотрим, есть\n",
    "    # ли она в одном из множеств. если\n",
    "    # есть, то прибавляем 1 к одной из\n",
    "    # переменных.\n",
    "    for l in review:\n",
    "        if l in unique[0]:\n",
    "            positive += 1\n",
    "        elif l in unique[1]:\n",
    "            negative += 1\n",
    "    # есть случаи, когда они равны\n",
    "    # нулю или равны друг другу.\n",
    "    # тогда считаем, что наш код не\n",
    "    # смог определить тональность.\n",
    "    if positive - negative == 0:\n",
    "        return None\n",
    "    # в остальных случаях все зависит\n",
    "    # от того, каких слов больше.\n",
    "    if positive < negative:\n",
    "        return \"negative\"\n",
    "    return \"positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность нашей модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:\n",
      "0.7216494845360825\n"
     ]
    }
   ],
   "source": [
    "# количество верных определений.\n",
    "correct = 0\n",
    "\n",
    "# количество случаев, когда машина\n",
    "# не нашла ответа.\n",
    "unknown = 0\n",
    "\n",
    "# для каждого из тестовых наборов\n",
    "# пишем такие циклы:\n",
    "for review in good_reviews_test:\n",
    "    if review_tone(review) == \"positive\":\n",
    "        correct += 1\n",
    "    elif not review_tone(review):\n",
    "        unknown += 1\n",
    "for review in bad_reviews_test:\n",
    "    if review_tone(review) == \"negative\":\n",
    "        correct += 1\n",
    "    elif not review_tone(review):\n",
    "        unknown += 1\n",
    "\n",
    "# результат теста:\n",
    "print(\"accuracy:\")\n",
    "print(correct/((len(good_reviews_test)+len(bad_reviews_test)) - unknown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Способы улучшить модель:\n",
    "* Во время тестирования брать из отзыва новые уникальные слова для первого сета и обновлять второй сет.\n",
    "* Выбрасывать уникальные слова, если они несколько раз встречаются в одном и том же отзыве и больше не встречаются нигде (в этом случае нам придется использовать не extend при сборе данных, а append). Параметры можно менять.\n",
    "* Учитывать частотности уникальных слов и не просто прибавлять по единице к positive и negative с каждым уникальным словом, а умножать ее на соответствующий коэффициент и только потом прибавлять.\n",
    "\n",
    "*Миша Сонькин*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
